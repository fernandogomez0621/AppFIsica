"""
============================================================================
APLICACI√ìN RAG INDEPENDIENTE
============================================================================

Sistema RAG educativo para aprender sobre M√°quinas de Boltzmann.
Esta aplicaci√≥n se ejecuta de forma independiente para evitar conflictos.

Autor: Sistema de F√≠sica
Versi√≥n: 1.0.0
"""

import os
import streamlit as st
import pandas as pd
from typing import List, Dict
import warnings

# --- Importaciones Problem√°ticas ---
# Se importan aqu√≠ para que los errores queden aislados
try:
    import fitz
    from langchain.schema import Document
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import Chroma
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from langchain_groq import ChatGroq
    from langchain.chains import RetrievalQA
    from langchain.prompts import PromptTemplate
    LANGCHAIN_OK = True
except ImportError as e:
    LANGCHAIN_OK = False
    LANGCHAIN_ERROR = e
# -----------------------------------

warnings.filterwarnings('ignore')

def render_rag_app():
    """Renderiza la aplicaci√≥n RAG completa"""
    st.set_page_config(page_title="üéì RAG - M√°quinas de Boltzmann", layout="wide")
    st.title("üéì Aprende sobre M√°quinas de Boltzmann")
    st.markdown("### *Sistema RAG con Papers Cient√≠ficos y Groq AI*")

    # Verificar API key y papers
    try:
        GROQ_API_KEY = st.secrets["GROQ_API_KEY"]
        api_configured = True
        st.success("‚úÖ API Key de Groq configurada")
    except:
        api_configured = False
        st.error("‚ùå API Key de Groq no encontrada en .streamlit/secrets.toml")

    pdf_files = [f for f in os.listdir("./articles/") if f.endswith(".pdf")]
    if pdf_files:
        st.success(f"‚úÖ {len(pdf_files)} papers encontrados en ./articles/")
    else:
        st.warning("‚ö†Ô∏è No se encontraron papers en ./articles/")

    # Verificar si LangChain est√° disponible
    if not LANGCHAIN_OK:
        st.error(f"‚ùå Error de dependencias: {LANGCHAIN_ERROR}")
        st.markdown("""
        **Aseg√∫rate de haber instalado las dependencias:**
        ```bash
        pip install "langchain<0.2" "langchain-community<0.1" "langchain-groq<0.2" "chromadb<0.5" "sentence-transformers<3" "pymupdf<1.24"
        ```
        """)
        return

    # Si todo est√° listo, mostrar el bot√≥n para inicializar
    if 'rag_system' not in st.session_state and api_configured and pdf_files:
        if st.button("üöÄ INICIALIZAR SISTEMA RAG", type="primary"):
            with st.spinner("‚ö° Inicializando sistema RAG... Esto puede tardar unos minutos."):
                try:
                    # 1. Procesar PDFs
                    documents = []
                    for pdf_file in pdf_files:
                        doc = fitz.open(os.path.join("./articles/", pdf_file))
                        for page_num, page in enumerate(doc, start=1):
                            documents.append(Document(page_content=page.get_text(), metadata={"source": pdf_file, "page": page_num}))
                    
                    # 2. Split
                    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
                    chunks = text_splitter.split_documents(documents)
                    
                    # 3. Embeddings
                    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
                    
                    # 4. Vector Store
                    vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory="./chroma_db_rag")
                    
                    # 5. LLM
                    llm = ChatGroq(groq_api_key=GROQ_API_KEY, model_name="llama-3.3-70b-versatile")
                    
                    # 6. QA Chain
                    prompt_template = """Contexto: {context}\n\nPregunta: {question}\n\nRespuesta:"""
                    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
                    
                    st.session_state.rag_system = RetrievalQA.from_chain_type(
                        llm=llm,
                        chain_type="stuff",
                        retriever=vectorstore.as_retriever(),
                        chain_type_kwargs={"prompt": PROMPT},
                        return_source_documents=True,
                    )
                    st.success("‚úÖ Sistema RAG inicializado exitosamente!")
                    st.rerun()

                except Exception as e:
                    st.error(f"‚ùå Error durante la inicializaci√≥n: {e}")

    # Si el sistema est√° listo, mostrar la interfaz de chat
    if 'rag_system' in st.session_state:
        st.success("ü§ñ **Sistema RAG Activo** - ¬°Haz tus preguntas!")
        if "messages" not in st.session_state:
            st.session_state.messages = []

        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        if prompt := st.chat_input("Pregunta sobre RBMs..."):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.chat_message("assistant"):
                with st.spinner("Consultando papers..."):
                    response = st.session_state.rag_system({"query": prompt})
                    st.markdown(response["result"])
                    
                    # Mostrar fuentes
                    sources = {doc.metadata['source'] for doc in response['source_documents']}
                    st.caption(f"Fuentes consultadas: {', '.join(sources)}")

            st.session_state.messages.append({"role": "assistant", "content": response["result"]})

if __name__ == "__main__":
    render_rag_app()