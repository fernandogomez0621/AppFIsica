====================================
8. Modelos Supervisados
====================================

Esta gu√≠a te ense√±ar√° a entrenar y comparar m√∫ltiples modelos de clasificaci√≥n supervisada para predecir el nivel de riesgo crediticio.

Objetivo del M√≥dulo
===================

El m√≥dulo de modelos supervisados te permite:

* ü§ñ **Entrenar 9 algoritmos** de clasificaci√≥n
* üìä **Comparar rendimiento** autom√°ticamente
* üéØ **Optimizar hiperpar√°metros** con Grid Search
* üìà **Visualizar m√©tricas** y curvas ROC
* üíæ **Guardar mejores modelos** para producci√≥n
* üîç **Analizar importancia** de features

Acceso al M√≥dulo
================

En el sidebar, click en:

.. code-block:: text

   ü§ñ Modelado ‚Üí ü§ñ Modelos Supervisados

Algoritmos Disponibles
=======================

1. Logistic Regression
-----------------------

**Caracter√≠sticas:**

* Simple y r√°pido
* Interpretable
* Baseline s√≥lido

**Cu√°ndo usar:** Relaciones lineales, interpretabilidad cr√≠tica.

2. Random Forest
----------------

**Caracter√≠sticas:**

* Robusto a outliers
* Maneja no linealidades
* Feature importance

**Cu√°ndo usar:** Datos complejos, alta precisi√≥n.

3. XGBoost
----------

**Caracter√≠sticas:**

* Estado del arte
* Muy preciso
* Maneja desbalanceo

**Cu√°ndo usar:** Competencias, m√°xima precisi√≥n.

4. LightGBM
-----------

**Caracter√≠sticas:**

* Muy r√°pido
* Eficiente en memoria
* Preciso

**Cu√°ndo usar:** Datasets grandes, velocidad cr√≠tica.

5. Support Vector Machine (SVM)
--------------------------------

**Caracter√≠sticas:**

* Kernel tricks
* Margen m√°ximo
* Robusto

**Cu√°ndo usar:** Datasets peque√±os/medianos, alta dimensionalidad.

6. K-Nearest Neighbors (KNN)
-----------------------------

**Caracter√≠sticas:**

* No param√©trico
* Simple
* Lazy learning

**Cu√°ndo usar:** Patrones locales, pocos datos.

7. Naive Bayes
--------------

**Caracter√≠sticas:**

* Muy r√°pido
* Probabil√≠stico
* Asume independencia

**Cu√°ndo usar:** Baseline r√°pido, features independientes.

8. Neural Network (MLP)
-----------------------

**Caracter√≠sticas:**

* Aprende no linealidades
* Flexible
* Requiere m√°s datos

**Cu√°ndo usar:** Patrones complejos, suficientes datos.

9. Gradient Boosting
--------------------

**Caracter√≠sticas:**

* Ensemble potente
* Preciso
* Interpretable

**Cu√°ndo usar:** Balance precisi√≥n/interpretabilidad.

Proceso de Entrenamiento
=========================

Paso 1: Preparar Datos
-----------------------

.. code-block:: text

   [‚öôÔ∏è Preparar Datos]
   
   ‚úì Features: 20 (incluyendo RBM)
   ‚úì Target: nivel_riesgo (3 clases)
   ‚úì Train: 8,000 (80%)
   ‚úì Test: 2,000 (20%)
   ‚úì Balanceo: SMOTE aplicado

Paso 2: Seleccionar Modelos
----------------------------

.. code-block:: text

   Modelos a entrenar:
   
   ‚òë Logistic Regression
   ‚òë Random Forest
   ‚òë XGBoost
   ‚òë LightGBM
   ‚òë SVM
   ‚òê KNN
   ‚òê Naive Bayes
   ‚òê Neural Network
   ‚òê Gradient Boosting

Paso 3: Entrenar Modelos
-------------------------

.. code-block:: text

   [üéØ Entrenar Modelos Seleccionados]
   
   Entrenando Logistic Regression... ‚úì (2s)
   Entrenando Random Forest... ‚úì (15s)
   Entrenando XGBoost... ‚úì (25s)
   Entrenando LightGBM... ‚úì (12s)
   Entrenando SVM... ‚úì (45s)
   
   ‚úì Todos los modelos entrenados
   Tiempo total: 1m 39s

Comparaci√≥n de Modelos
======================

Tabla de M√©tricas
-----------------

.. list-table::
   :header-rows: 1
   :widths: 25 15 15 15 15 15

   * - Modelo
     - Accuracy
     - Precision
     - Recall
     - F1-Score
     - ROC-AUC
   * - XGBoost
     - **95.2%**
     - 94.8%
     - 95.1%
     - **95.0%**
     - **0.982**
   * - LightGBM
     - 94.8%
     - **95.1%**
     - 94.5%
     - 94.8%
     - 0.979
   * - Random Forest
     - 94.1%
     - 93.8%
     - **95.3%**
     - 94.5%
     - 0.975
   * - SVM
     - 92.5%
     - 92.1%
     - 92.8%
     - 92.4%
     - 0.968
   * - Logistic Reg.
     - 89.3%
     - 88.7%
     - 89.9%
     - 89.3%
     - 0.945

**Mejor modelo:** XGBoost (95.2% accuracy)

Matriz de Confusi√≥n
-------------------

Para XGBoost:

.. code-block:: text

                Predicho
              Bajo  Medio  Alto
   Real Bajo  1180    25     5
        Medio   30   570    15
        Alto     8    12   155
   
   Accuracy: 95.2%
   Errores: 95 de 2,000

Curvas ROC
----------

.. code-block:: text

   [üìä Visualizar Curvas ROC]
   
   Curva ROC multiclase (One-vs-Rest)
   ‚Ä¢ Bajo vs Rest: AUC = 0.985
   ‚Ä¢ Medio vs Rest: AUC = 0.978
   ‚Ä¢ Alto vs Rest: AUC = 0.983

Importancia de Features
=======================

Top 10 Features
---------------

Para Random Forest:

.. code-block:: text

   Feature Importance:
   
   1. dti: 0.185 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
   2. puntaje_datacredito: 0.152 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
   3. capacidad_residual: 0.128 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
   4. ltv: 0.095 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
   5. rbm_feature_1: 0.082 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
   6. score_estabilidad: 0.071 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
   7. patrimonio_total: 0.065 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
   8. salario_mensual: 0.058 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
   9. rbm_feature_5: 0.052 ‚ñà‚ñà‚ñà‚ñà‚ñà
   10. edad: 0.048 ‚ñà‚ñà‚ñà‚ñà

**Insights:**

* DTI es el predictor m√°s importante
* Features RBM aportan valor significativo
* Variables financieras dominan

SHAP Values
-----------

.. code-block:: text

   [üîç Calcular SHAP Values]
   
   An√°lisis de contribuci√≥n individual
   Muestra c√≥mo cada feature afecta predicciones

Optimizaci√≥n de Hiperpar√°metros
================================

Grid Search
-----------

.. code-block:: text

   Modelo: XGBoost
   
   Par√°metros a optimizar:
   ‚Ä¢ n_estimators: [100, 200, 300]
   ‚Ä¢ max_depth: [3, 5, 7]
   ‚Ä¢ learning_rate: [0.01, 0.05, 0.1]
   ‚Ä¢ subsample: [0.8, 0.9, 1.0]
   
   [üîß Ejecutar Grid Search]
   
   Probando 36 combinaciones...
   Mejor combinaci√≥n encontrada:
   ‚Ä¢ n_estimators: 200
   ‚Ä¢ max_depth: 5
   ‚Ä¢ learning_rate: 0.05
   ‚Ä¢ subsample: 0.9
   
   Accuracy mejorada: 95.2% ‚Üí 95.8%

Random Search
-------------

M√°s r√°pido que Grid Search:

.. code-block:: text

   [üé≤ Ejecutar Random Search]
   
   100 iteraciones aleatorias
   Tiempo: 15 minutos
   Mejora: +0.4% accuracy

Validaci√≥n Cruzada
==================

K-Fold Cross-Validation
-----------------------

.. code-block:: text

   [‚úÖ Validaci√≥n Cruzada (5-fold)]
   
   Fold 1: 94.8%
   Fold 2: 95.1%
   Fold 3: 95.5%
   Fold 4: 94.9%
   Fold 5: 95.3%
   
   Media: 95.1% ¬± 0.3%
   
   Modelo robusto ‚úì

Stratified K-Fold
-----------------

Mantiene proporci√≥n de clases:

.. code-block:: text

   Distribuci√≥n preservada en cada fold
   Mejor para datasets desbalanceados

Manejo de Desbalanceo
=====================

T√©cnicas Disponibles
--------------------

**1. SMOTE (Synthetic Minority Over-sampling)**

.. code-block:: text

   Antes:
   Bajo: 6,074 (60.7%)
   Medio: 2,943 (29.4%)
   Alto: 983 (9.8%)
   
   Despu√©s de SMOTE:
   Bajo: 6,074 (33.3%)
   Medio: 6,074 (33.3%)
   Alto: 6,074 (33.3%)

**2. Class Weights**

.. code-block:: text

   Pesos autom√°ticos:
   Bajo: 0.55
   Medio: 1.13
   Alto: 3.39

**3. Undersampling**

Reduce clase mayoritaria.

**4. Ensemble Methods**

Combina m√∫ltiples t√©cnicas.

Guardar Modelos
===============

Guardar Mejor Modelo
---------------------

.. code-block:: text

   [üíæ Guardar Modelo]
   
   Modelo: XGBoost
   Accuracy: 95.8%
   
   Guardado:
   ‚úì Modelo entrenado
   ‚úì Scaler
   ‚úì Feature names
   ‚úì M√©tricas
   ‚úì Hiperpar√°metros
   
   Archivo: xgboost_best_20240115.pkl

Cargar Modelo
-------------

.. code-block:: text

   [üìÇ Cargar Modelo]
   
   ‚úì Modelo cargado
   ‚úì Listo para predicciones

Casos de Uso
============

**Caso 1: Producci√≥n**

Entrenar modelo final para sistema en vivo.

**Caso 2: Comparaci√≥n**

Evaluar m√∫ltiples algoritmos para seleccionar el mejor.

**Caso 3: Investigaci√≥n**

Experimentar con diferentes features y configuraciones.

Tips y Mejores Pr√°cticas
=========================

‚úÖ **Haz:**

- Entrena m√∫ltiples modelos
- Usa validaci√≥n cruzada
- Optimiza hiperpar√°metros
- Maneja desbalanceo
- Guarda modelos y m√©tricas

‚ùå **Evita:**

- Overfitting (validar siempre)
- Ignorar desbalanceo de clases
- No optimizar hiperpar√°metros
- Entrenar sin normalizar
- Usar solo accuracy como m√©trica

Troubleshooting
===============

**Problema: Overfitting**

Soluci√≥n: Regularizaci√≥n, m√°s datos, validaci√≥n cruzada.

**Problema: Underfitting**

Soluci√≥n: Modelo m√°s complejo, m√°s features, menos regularizaci√≥n.

**Problema: Entrenamiento lento**

Soluci√≥n: LightGBM, menos datos, menos hiperpar√°metros.

Pr√≥ximos Pasos
==============

Con tus modelos entrenados:

1. **Predecir**: :doc:`09_prediccion`
2. **Reentrenar**: :doc:`10_reentrenamiento`
3. **Aprender**: :doc:`11_rag_educativo`

¬°Modelos listos para producci√≥n! ü§ñ