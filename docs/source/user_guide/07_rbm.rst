===============================================
7. MÃ¡quina de Boltzmann Restringida (RBM)
===============================================

Esta guÃ­a te enseÃ±arÃ¡ a entrenar y usar MÃ¡quinas de Boltzmann Restringidas para extracciÃ³n de caracterÃ­sticas latentes en datos de crÃ©dito hipotecario.

Objetivo del MÃ³dulo
===================

El mÃ³dulo de RBM te permite:

* âš¡ **Entrenar RBM** desde cero
* ğŸ§  **Extraer caracterÃ­sticas latentes** no lineales
* ğŸ“Š **Visualizar pesos** y activaciones
* ğŸ“ˆ **Monitorear convergencia** durante entrenamiento
* ğŸ’¾ **Guardar modelos** entrenados
* ğŸ”„ **Usar features** en modelos supervisados

Â¿QuÃ© es una RBM?
================

DefiniciÃ³n
----------

Una **MÃ¡quina de Boltzmann Restringida** es un modelo generativo no supervisado que:

* Aprende representaciones latentes de los datos
* Usa una arquitectura de dos capas (visible + oculta)
* Se entrena con **Contrastive Divergence (CD-k)**
* Extrae caracterÃ­sticas Ãºtiles para modelos supervisados

Arquitectura
------------

.. code-block:: text

   Capa Oculta (h)
   â—‹ â—‹ â—‹ â—‹ â—‹ â—‹ â—‹ â—‹ â—‹ â—‹  (100 unidades)
   â†• â†• â†• â†• â†• â†• â†• â†• â†• â†•
   â— â— â— â— â— â— â— â— â— â—  (n_features unidades)
   Capa Visible (v)

**CaracterÃ­sticas:**

* Sin conexiones dentro de cada capa (restricciÃ³n)
* Conexiones completas entre capas
* Pesos bidireccionales W
* Sesgos a (visible) y b (oculto)

FunciÃ³n de EnergÃ­a
------------------

.. math::

   E(v,h) = -\sum_i a_i v_i - \sum_j b_j h_j - \sum_{i,j} v_i W_{ij} h_j

Donde:

* :math:`v_i` son las unidades visibles
* :math:`h_j` son las unidades ocultas  
* :math:`W_{ij}` son los pesos de conexiÃ³n
* :math:`a_i, b_j` son los sesgos

Acceso al MÃ³dulo
================

En el sidebar, click en:

.. code-block:: text

   ğŸ¤– Modelado â†’ âš¡ MÃ¡quina de Boltzmann (RBM)

ConfiguraciÃ³n de HiperparÃ¡metros
=================================

ParÃ¡metros Principales
----------------------

**1. NÃºmero de Unidades Ocultas**

.. code-block:: text

   Unidades ocultas: [100]
   Rango: 50 - 500

**Â¿QuÃ© significa?**

El nÃºmero de caracterÃ­sticas latentes que la RBM aprenderÃ¡.

**Recomendaciones:**

* **50-100**: Datasets pequeÃ±os (<5K registros)
* **100-200**: Datasets medianos (5K-20K)
* **200-500**: Datasets grandes (>20K)

.. tip::
   Comienza con 100 y ajusta segÃºn resultados.

**2. Learning Rate (Tasa de Aprendizaje)**

.. code-block:: text

   Learning rate: [0.01]
   Rango: 0.001 - 0.1

**Â¿QuÃ© significa?**

QuÃ© tan rÃ¡pido el modelo aprende de los datos.

**Recomendaciones:**

* **0.001-0.005**: Aprendizaje lento pero estable
* **0.01**: Valor por defecto balanceado
* **0.05-0.1**: Aprendizaje rÃ¡pido pero inestable

.. warning::
   Learning rate muy alto puede causar divergencia.

**3. NÃºmero de Ã‰pocas**

.. code-block:: text

   Ã‰pocas: [100]
   Rango: 50 - 500

**Â¿QuÃ© significa?**

CuÃ¡ntas veces el modelo ve todo el dataset.

**Recomendaciones:**

* **50-100**: Pruebas rÃ¡pidas
* **100-200**: Entrenamiento estÃ¡ndar
* **200-500**: Entrenamiento exhaustivo

**4. Batch Size**

.. code-block:: text

   Batch size: [64]
   Rango: 32 - 256

**Â¿QuÃ© significa?**

CuÃ¡ntos ejemplos se procesan simultÃ¡neamente.

**Recomendaciones:**

* **32-64**: Datasets pequeÃ±os, mÃ¡s estable
* **64-128**: Valor estÃ¡ndar
* **128-256**: Datasets grandes, mÃ¡s rÃ¡pido

**5. Pasos de Contrastive Divergence (k)**

.. code-block:: text

   CD-k steps: [1]
   Rango: 1 - 10

**Â¿QuÃ© significa?**

CuÃ¡ntos pasos de Gibbs sampling se ejecutan.

**Recomendaciones:**

* **k=1**: MÃ¡s rÃ¡pido, suficiente para mayorÃ­a de casos
* **k=5**: MÃ¡s preciso pero mÃ¡s lento
* **k=10**: Solo para investigaciÃ³n

Proceso de Entrenamiento
=========================

Paso 1: Preparar Datos
-----------------------

.. code-block:: text

   [âš™ï¸ Preparar Datos para RBM]
   
   âœ“ NormalizaciÃ³n aplicada (StandardScaler)
   âœ“ Variables seleccionadas: 15
   âœ“ Registros de entrenamiento: 8,000
   âœ“ Registros de validaciÃ³n: 2,000

Paso 2: Configurar HiperparÃ¡metros
-----------------------------------

.. code-block:: text

   ConfiguraciÃ³n:
   â€¢ Unidades ocultas: 100
   â€¢ Learning rate: 0.01
   â€¢ Ã‰pocas: 100
   â€¢ Batch size: 64
   â€¢ CD-k: 1

Paso 3: Entrenar RBM
---------------------

.. code-block:: text

   [ğŸ¯ Entrenar RBM]
   
   Ã‰poca 1/100: Loss = 125.3
   Ã‰poca 10/100: Loss = 89.2
   Ã‰poca 20/100: Loss = 67.5
   Ã‰poca 30/100: Loss = 54.8
   ...
   Ã‰poca 100/100: Loss = 23.1
   
   âœ“ Entrenamiento completado
   Tiempo total: 2m 15s

Monitoreo del Entrenamiento
============================

Curva de Aprendizaje
---------------------

GrÃ¡fico que muestra la evoluciÃ³n del error:

.. code-block:: text

   Loss vs Ã‰pocas
   
   150 â”¤
       â”‚ â—
   100 â”¤  â—â—
       â”‚    â—â—â—
    50 â”¤       â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
       â”‚
     0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       0    25    50    75   100
            Ã‰pocas

**InterpretaciÃ³n:**

* Descenso rÃ¡pido inicial: Aprendizaje efectivo
* EstabilizaciÃ³n: Convergencia alcanzada
* Oscilaciones: Posible learning rate alto

ReconstrucciÃ³n de Datos
------------------------

Compara datos originales vs reconstruidos:

.. code-block:: text

   Error de ReconstrucciÃ³n:
   
   Media: 0.023
   Mediana: 0.019
   MÃ¡ximo: 0.145
   
   Calidad: Excelente âœ“

VisualizaciÃ³n de Pesos
======================

Matriz de Pesos
---------------

.. code-block:: text

   [ğŸ¨ Visualizar Matriz de Pesos]
   
   Heatmap 15x100
   â€¢ Filas: Features de entrada
   â€¢ Columnas: Unidades ocultas
   â€¢ Color: Magnitud del peso

**InterpretaciÃ³n:**

* Pesos altos (rojo): ConexiÃ³n fuerte
* Pesos bajos (azul): ConexiÃ³n dÃ©bil
* Patrones: Grupos de features relacionadas

Filtros Aprendidos
------------------

Visualiza quÃ© aprende cada unidad oculta:

.. code-block:: text

   Unidad Oculta 1:
   Detecta: Alto salario + Bajo DTI
   
   Unidad Oculta 2:
   Detecta: Buen puntaje + Alta capacidad residual
   
   Unidad Oculta 3:
   Detecta: Patrimonio alto + MÃºltiples propiedades

ExtracciÃ³n de CaracterÃ­sticas
==============================

Transformar Datos
-----------------

.. code-block:: text

   [ğŸ”„ Extraer Features con RBM]
   
   Datos originales: 10,000 Ã— 15
   Features RBM: 10,000 Ã— 100
   
   âœ“ TransformaciÃ³n completada

**Uso de Features:**

Las 100 caracterÃ­sticas latentes pueden usarse como entrada para:

* Modelos supervisados (Random Forest, XGBoost, etc.)
* Clustering mejorado
* ReducciÃ³n de dimensionalidad

AnÃ¡lisis de Activaciones
-------------------------

.. code-block:: text

   EstadÃ­sticas de Activaciones:
   
   Media: 0.45
   Desv. Est.: 0.28
   Sparsity: 35% (unidades inactivas)
   
   DistribuciÃ³n: Balanceada âœ“

Casos de Uso
============

Caso 1: Pre-entrenamiento para Deep Learning
---------------------------------------------

**Objetivo**: Inicializar pesos de red neuronal profunda.

**Pasos:**

1. Entrenar RBM en datos no etiquetados
2. Usar pesos como inicializaciÃ³n
3. Fine-tuning con backpropagation
4. Mejor convergencia y generalizaciÃ³n

Caso 2: Feature Engineering AutomÃ¡tico
---------------------------------------

**Objetivo**: Crear features no lineales automÃ¡ticamente.

**Pasos:**

1. Entrenar RBM con 200 unidades ocultas
2. Extraer activaciones como nuevas features
3. Combinar con features originales
4. Entrenar Random Forest
5. Mejora de 3-5% en precisiÃ³n

Caso 3: DetecciÃ³n de AnomalÃ­as
-------------------------------

**Objetivo**: Identificar solicitudes atÃ­picas.

**Pasos:**

1. Entrenar RBM en datos normales
2. Calcular error de reconstrucciÃ³n
3. Umbral: error > percentil 95
4. Marcar como anomalÃ­as

Caso 4: ReducciÃ³n de Dimensionalidad
-------------------------------------

**Objetivo**: Reducir de 47 a 20 features.

**Pasos:**

1. Entrenar RBM con 20 unidades ocultas
2. Extraer activaciones
3. Usar en lugar de features originales
4. Mantener 90% de informaciÃ³n

Tips y Mejores PrÃ¡cticas
=========================

SelecciÃ³n de HiperparÃ¡metros
-----------------------------

âœ… **Haz:**

* Comienza con valores por defecto
* Usa validaciÃ³n cruzada
* Monitorea curva de aprendizaje
* Ajusta learning rate si no converge

âŒ **Evita:**

* Learning rate muy alto (>0.1)
* Demasiadas unidades ocultas (overfitting)
* Muy pocas Ã©pocas (<50)
* Ignorar normalizaciÃ³n de datos

OptimizaciÃ³n del Entrenamiento
-------------------------------

**Para acelerar:**

* Aumenta batch size (128-256)
* Reduce Ã©pocas inicialmente
* Usa CD-1 en lugar de CD-5
* Entrena en GPU si disponible

**Para mejorar calidad:**

* Aumenta Ã©pocas (200-300)
* Reduce learning rate (0.005)
* Usa CD-5 o CD-10
* Aumenta unidades ocultas

InterpretaciÃ³n de Resultados
-----------------------------

**SeÃ±ales de buen entrenamiento:**

* Loss decrece consistentemente
* Error de reconstrucciÃ³n < 0.05
* Activaciones balanceadas (30-70%)
* Pesos no explotan ni desaparecen

**SeÃ±ales de problemas:**

* Loss oscila o aumenta
* Error de reconstrucciÃ³n > 0.1
* Todas las unidades activas/inactivas
* Pesos muy grandes (>10) o muy pequeÃ±os (<0.01)

Guardar y Cargar Modelos
=========================

Guardar Modelo
--------------

.. code-block:: text

   [ğŸ’¾ Guardar Modelo RBM]
   
   Guardado:
   âœ“ Pesos W
   âœ“ Sesgos a, b
   âœ“ HiperparÃ¡metros
   âœ“ Scaler
   
   Archivo: rbm_model_20240115.pkl

Cargar Modelo
-------------

.. code-block:: text

   [ğŸ“‚ Cargar Modelo RBM]
   
   Selecciona archivo: rbm_model_20240115.pkl
   
   âœ“ Modelo cargado
   âœ“ Listo para transformar datos

Troubleshooting
===============

Problema 1: Loss no decrece
----------------------------

**SÃ­ntomas:**

.. code-block:: text

   Ã‰poca 1: Loss = 125.3
   Ã‰poca 50: Loss = 124.8
   Ã‰poca 100: Loss = 124.5

**Causas posibles:**

* Learning rate muy bajo
* Datos no normalizados
* InicializaciÃ³n pobre

**Soluciones:**

1. Aumenta learning rate a 0.05
2. Verifica normalizaciÃ³n (media=0, std=1)
3. Reinicia con nueva semilla aleatoria

Problema 2: Loss explota
-------------------------

**SÃ­ntomas:**

.. code-block:: text

   Ã‰poca 1: Loss = 125.3
   Ã‰poca 5: Loss = 450.2
   Ã‰poca 10: Loss = NaN

**Causas posibles:**

* Learning rate muy alto
* Gradientes explotan
* Datos con outliers extremos

**Soluciones:**

1. Reduce learning rate a 0.001
2. Usa gradient clipping
3. Elimina outliers extremos

Problema 3: Overfitting
-----------------------

**SÃ­ntomas:**

.. code-block:: text

   Train loss: 15.2
   Val loss: 45.8

**Soluciones:**

1. Reduce unidades ocultas
2. Agrega regularizaciÃ³n L2
3. Usa dropout
4. Aumenta datos de entrenamiento

Problema 4: Entrenamiento muy lento
------------------------------------

**Soluciones:**

1. Aumenta batch size a 128
2. Reduce Ã©pocas a 50
3. Usa CD-1 en lugar de CD-5
4. Reduce nÃºmero de features

ComparaciÃ³n con Otros MÃ©todos
==============================

.. list-table::
   :header-rows: 1
   :widths: 25 25 25 25

   * - MÃ©todo
     - Velocidad
     - Calidad
     - Interpretabilidad
   * - RBM
     - Media
     - Alta
     - Media
   * - PCA
     - RÃ¡pida
     - Media
     - Alta
   * - Autoencoder
     - Lenta
     - Muy Alta
     - Baja
   * - t-SNE
     - Muy Lenta
     - Alta
     - Baja

PrÃ³ximos Pasos
==============

Con tu RBM entrenada:

1. **Usar features**: :doc:`08_modelos_supervisados`
2. **Predecir riesgo**: :doc:`09_prediccion`
3. **Aprender mÃ¡s**: :doc:`11_rag_educativo`

Â¡RBM entrenada exitosamente! âš¡